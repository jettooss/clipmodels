{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f1f90-b04b-4e7f-8a98-dc0b9a73e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from einops import repeat\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B, T_q, _ = q.size()\n",
    "        T_k = k.size(1)\n",
    "        Q = self.q_proj(q).view(B, T_q, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(k).view(B, T_k, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(v).view(B, T_k, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        scores = (Q @ K.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            # mask: (B, T_k) -> (B, 1, 1, T_k)\n",
    "            attn_mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        ctx = (attn @ V).transpose(1,2).contiguous().view(B, T_q, self.d_model)\n",
    "        return self.out_proj(ctx)\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model, eps=eps)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, sub):\n",
    "        return self.norm(x + self.dropout(sub))\n",
    "\n",
    "class PositionwiseFFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:,1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:,:x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.an1 = AddNorm(d_model, dropout)\n",
    "        self.ff = PositionwiseFFN(d_model, d_ff, dropout)\n",
    "        self.an2 = AddNorm(d_model, dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.an1(x, self.sa(x, x, x, mask))\n",
    "        x = self.an2(x, self.ff(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=4, emb_size=32):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Unfold(kernel_size=patch_size, stride=patch_size),\n",
    "            nn.Linear(patch_size*patch_size*in_channels, emb_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        patches = self.projection[0](x)\n",
    "        patches = patches.transpose(1,2)\n",
    "        return self.projection[1](patches)\n",
    "\n",
    "class TextTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, num_heads=4, d_ff=1024,\n",
    "                 num_layers=3, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed   = PositionalEncoding(d_model, max_len, dropout)\n",
    "        self.layers      = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, tokens, mask=None):\n",
    "        x = self.token_embed(tokens)\n",
    "        x = self.pos_embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)[:,0,:]\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, ch=3, img_size=144, patch_size=4, emb_dim=32,\n",
    "                 n_layers=6, heads=2, d_ff=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(ch, patch_size, emb_dim)\n",
    "        num_patches = (img_size//patch_size)**2\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, emb_dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,emb_dim))\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(emb_dim, heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "    def forward(self, img):\n",
    "        x = self.patch_embedding(img)\n",
    "        b,n,_ = x.shape\n",
    "        cls = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
    "        x = torch.cat([cls, x], dim=1) + self.pos_embedding[:,:n+1]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39739d-bf2f-468c-a93b-8b4c29c91e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    " class CLIP(nn.Module):\n",
    "    def __init__(self, vision_encoder, text_encoder, embed_dim=512, temp=0.07):\n",
    "        super().__init__()\n",
    "        self.vision = vision_encoder\n",
    "        self.text   = text_encoder\n",
    "        self.vision_proj = nn.Linear(vision_encoder.norm.normalized_shape[0], embed_dim)\n",
    "        self.text_proj   = nn.Linear(text_encoder.norm.normalized_shape[0], embed_dim)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([])*math.log(1/temp))\n",
    "    def forward(self, images, tokens, mask=None):\n",
    "        img_feats = self.vision(images)\n",
    "        txt_feats = self.text(tokens, mask)\n",
    "        img_emb = F.normalize(self.vision_proj(img_feats), dim=-1)\n",
    "        txt_emb = F.normalize(self.text_proj(txt_feats), dim=-1)\n",
    "        scale = self.logit_scale.exp()\n",
    "        logits_i = scale * img_emb @ txt_emb.t()\n",
    "        return logits_i, logits_i.t()\n",
    "    def contrastive_loss(self, logits_i):\n",
    "        B = logits_i.size(0)\n",
    "        targets = torch.arange(B, device=logits_i.device)\n",
    "        return (F.cross_entropy(logits_i, targets) + F.cross_entropy(logits_i.t(), targets))/2\n",
    "\n",
    "class CocoCLIPDataset(Dataset):\n",
    "    def __init__(self, img_folder, ann_file, tokenizer, max_length=16):\n",
    "        self.coco = CocoCaptions(root=img_folder, annFile=ann_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((144,144)),transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.coco)\n",
    "    def __getitem__(self, idx):\n",
    "        img, caps = self.coco[idx]\n",
    "        img = self.transform(img)\n",
    "        cap = random.choice(caps)\n",
    "        toks = self.tokenizer(cap, padding='max_length', truncation=True,\n",
    "                             max_length=self.max_length, return_tensors='pt')\n",
    "        return img, toks.input_ids.squeeze(0), toks.attention_mask.squeeze(0)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, ids, masks = zip(*batch)\n",
    "    return torch.stack(imgs), torch.stack(ids), torch.stack(masks)\n",
    "\n",
    "def train():\n",
    "    root = r'D:\\coco_dataset\\train2017'\n",
    "    train_img = os.path.join(root,'train2017')\n",
    "    val_img   = os.path.join(root,'val2017')\n",
    "    train_ann = os.path.join(root,'captions_train2017.json')\n",
    "    val_ann   = os.path.join(root,'captions_val2017.json')\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "    model = CLIP(\n",
    "        vision_encoder=VisionTransformer(),\n",
    "        text_encoder=TextTransformer(tokenizer.vocab_size),\n",
    "        embed_dim=512\n",
    "    )\n",
    "    model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    train_ds = CocoCLIPDataset(train_img, train_ann, tokenizer)\n",
    "    val_ds   = CocoCLIPDataset(val_img,   val_ann,   tokenizer)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,\n",
    "                               collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, 11):\n",
    "        model.train()\n",
    "        tbar = tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\")\n",
    "        run_loss = 0.0\n",
    "        for imgs, ids, masks in tbar:\n",
    "            imgs, ids, masks = imgs.to(device), ids.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits_i, logits_t = model(imgs, ids, mask=masks)\n",
    "                loss = model.contrastive_loss(logits_i)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            run_loss += loss.item()\n",
    "        model.eval()\n",
    "        val_loss, total = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, ids, masks in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\"):\n",
    "                imgs, ids, masks = imgs.to(device), ids.to(device), masks.to(device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits_i, _ = model(imgs, ids, mask=masks)\n",
    "                    l = model.contrastive_loss(logits_i).item()\n",
    "                bs = imgs.size(0)\n",
    "                val_loss += l * bs\n",
    "                total += bs\n",
    "        avg = val_loss / total\n",
    "        print(f\"==> Epoch {epoch} | Val Loss: {avg:.4f}\")\n",
    "        if avg < best_loss:\n",
    "            best_loss = avg\n",
    "            torch.save(model.state_dict(), 'best_clip_coco.pth')\n",
    "            print(\"Saved best model.\")\n",
    "\n",
    " \n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cfe410-9aef-4c3d-95c3-8882590777dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
